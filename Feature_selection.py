# -*- coding: utf-8 -*-
"""SFS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e70ZGRibODJNEPcUgiiHjk9szyBxTkWZ
"""

#importing libraries
import numpy as np
import pandas as pd
from sklearn import metrics]
from sklearn.model_selection import train_test_split, StratifiedKFold,RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.metrics import classification_report



#Reading dataset using pandas
train_data = pd.read_csv("train_data.csv")

#Missing value Imputation: Complete case deletion
train_data.dropna(how='any', inplace=True)

# Making independant features separate
X = train_data.iloc[:, 1:-1]

# Deletning irrelevant feature
X = X.drop(["patientid"], axis = 1)

# Target Feature
y = train_data.iloc[:, -1]

# Simple train-test split
X, X_test, y, y_test = train_test_split(X, y, test_size = 0.33, random_state = 7)


# Encoding target column
le_Stay = LabelEncoder()
y = le_Stay.fit_transform(y)
y_test = le_Stay.fit_transform(y_test)

# Encoding independant features
objectColumns = []
for col in X:
    if X[col].dtype.name == 'object':
        X[col] = X[col].astype('category')
        X[col] = X[col].cat.codes
        X_test[col] = X_test[col].astype('category')
        X_test[col] = X_test[col].cat.codes
        objectColumns.append(col)


# Normalization of train and test data separately
sc = MinMaxScaler()
X[["Admission_Deposit"]] = sc.fit_transform(X[["Admission_Deposit"]])
X[["Visitors with Patient"]] = sc.fit_transform(X[["Visitors with Patient"]])
X[["City_Code_Patient"]] = sc.fit_transform(X[["City_Code_Patient"]])
X[["Bed Grade"]] = sc.fit_transform(X[["Bed Grade"]])
X[["Available Extra Rooms in Hospital"]] = sc.fit_transform(X[["Available Extra Rooms in Hospital"]])
X[["City_Code_Hospital"]] = sc.fit_transform(X[["City_Code_Hospital"]])
X[["Hospital_code"]] = sc.fit_transform(X[["Hospital_code"]])

X_test[["Admission_Deposit"]] = sc.fit_transform(X_test[["Admission_Deposit"]])
X_test[["Visitors with Patient"]] = sc.fit_transform(X_test[["Visitors with Patient"]])
X_test[["City_Code_Patient"]] = sc.fit_transform(X_test[["City_Code_Patient"]])
X_test[["Bed Grade"]] = sc.fit_transform(X_test[["Bed Grade"]])
X_test[["Available Extra Rooms in Hospital"]] = sc.fit_transform(X_test[["Available Extra Rooms in Hospital"]])
X_test[["City_Code_Hospital"]] = sc.fit_transform(X_test[["City_Code_Hospital"]])
X_test[["Hospital_code"]] = sc.fit_transform(X_test[["Hospital_code"]])

#SFS Using Decision Tree classifier
sfs_dt = SFS(DecisionTreeClassifier(random_state=0,criterion='gini',max_depth=5),
          k_features=(1,6),forward=True,floating=False,verbose=0,n_jobs=-1,scoring='accuracy',cv=3
          ).fit(X,y)
print(list(sfs_dt.k_feature_names_))

#SFS Using Random Forest classifier
sfs_rf = SFS(RandomForestClassifier(n_estimators = 30, criterion = 'gini', random_state = 0),
          k_features=(1,6),forward=True,floating=False,verbose=0,n_jobs=-1,scoring='accuracy',cv=3
          ).fit(X,y)
print(list(sfs_rf.k_feature_names_))

#SFS Using Gaussian Naive Bayes classifier
sfs_nb = SFS(GaussianNB(),
          k_features=(1,6),forward=True,floating=False,verbose=0,n_jobs=-1,scoring='accuracy',cv=3
          ).fit(X,y)
print(list(sfs_nb.k_feature_names_))

def get_score(model, X_train, X_test, y_train, y_test):
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  return accuracy_score(y_pred, y_test)

#Making separate dataset for different classification techniques
X_dt = X[list(sfs_dt.k_feature_names_)]
X_test_dt = X_test[list(sfs_dt.k_feature_names_)]

X_rf = X[list(sfs_rf.k_feature_names_)]
X_test_rf = X_test[list(sfs_rf.k_feature_names_)]

X_nb = X[list(sfs_nb.k_feature_names_)]
X_test_nb = X_test[list(sfs_nb.k_feature_names_)]

#Storing accuracy for every classifier
accuracy_dec_tree = []
accuracy_random_forest = []
accuracy_naivebayes = []

accuracy_dec_tree.append( get_score(DecisionTreeClassifier(criterion = 'entropy', random_state = 0),X_dt, X_test_dt, y, y_test) )
accuracy_random_forest.append( get_score(RandomForestClassifier(n_estimators = 30, criterion = 'entropy', random_state = 0),X_rf, X_test_rf, y, y_test) )
accuracy_naivebayes.append( get_score(GaussianNB(),X_nb, X_test_nb, y, y_test) )

#Printing accuracy for different classifier
print(np.mean(accuracy_dec_tree))
print(np.mean(accuracy_random_forest))
print(np.mean(accuracy_naivebayes))